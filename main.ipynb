{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una instancia de GoogleAuth y autentica\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "# Crea una instancia de GoogleDrive\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ReadCSVFromDrive\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import csv\n",
    "import tempfile\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-Oct.csv\n",
      "2020-Jan.csv\n",
      "2019-Dec.csv\n",
      "2019-Nov.csv\n"
     ]
    }
   ],
   "source": [
    "# ID de la carpeta que deseas listar\n",
    "folder_id = \"1I8bgqZV-LaKKlT8UvCxeobSsT4y4VuFd\"\n",
    "\n",
    "# Lista de archivos en la carpeta\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "# Definir las columnas del DataFrame vacío\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"category_id\", StringType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Creo dataframe que contendra a todos los dataframes leidos del drive\n",
    "df_all = spark.createDataFrame([], schema)\n",
    "\n",
    "for file in file_list:\n",
    "    \n",
    "    if file['mimeType'] == 'text/csv':\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        print(file['originalFilename'])\n",
    "        try:\n",
    "            file.GetContentFile(temp_file.name)\n",
    "            \n",
    "            # Leer el contenido del archivo CSV y crear un DataFrame de PySpark\n",
    "            with open(temp_file.name, 'r') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile)\n",
    "                # Saltar la primera fila si contiene encabezados\n",
    "                next(csv_reader)\n",
    "                \n",
    "                # Crear una lista de filas para el DataFrame\n",
    "                rows = [row for row in csv_reader]\n",
    "                \n",
    "                # Crear el DataFrame de PySpark\n",
    "                df = spark.createDataFrame(rows )\n",
    "                df = df.selectExpr(\n",
    "                    \"_1 as id\",\n",
    "                    \"_2 as event_time\",\n",
    "                    \"_3 as event_type\",\n",
    "                    \"_4 as product_id\",\n",
    "                    \"_5 as category_id\",\n",
    "                    \"_6 as category_code\",\n",
    "                    \"_7 as brand\",\n",
    "                    \"_8 as price\",\n",
    "                    \"_9 as user_id\",\n",
    "                    \"_10 as user_session\"\n",
    "                )\n",
    "                #Agrego dataframe a lista\n",
    "                df_all = df_all.union(df) if df_all else df \n",
    "                # Muestra el DataFrame resultante\n",
    "                #df.show()\n",
    "                    \n",
    "        finally:\n",
    "            temp_file.close()\n",
    "# Detener la sesión de Spark al finalizar\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas: 2000000\n"
     ]
    }
   ],
   "source": [
    "# Contar la cantidad de filas en el DataFrame\n",
    "num_rows = df_all.count()\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(\"Cantidad de filas:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  brand|count|\n",
      "+-------+-----+\n",
      "|samsung|62074|\n",
      "|  apple|49605|\n",
      "| xiaomi|36168|\n",
      "| huawei|11758|\n",
      "|lucente| 7634|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cuenta la cantidad de ocurrencias de cada marca\n",
    "#brand_counts = df.groupBy(\"brand\").count()\n",
    "brand_counts = df.where(col(\"brand\") != '' ).groupBy(\"brand\").count()\n",
    "\n",
    "# Muestra el resultado\n",
    "brand_counts.orderBy(col('count').desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de PySpark a un DataFrame de Pandas para visualización\n",
    "brand_counts_pandas = brand_counts.toPandas()\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(brand_counts_pandas['brand'], brand_counts_pandas['count'])\n",
    "plt.xlabel('Marca')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.title('Recuento de Marcas')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|event_type| count|\n",
      "+----------+------+\n",
      "|      view|482642|\n",
      "|  purchase|  9595|\n",
      "|      cart|  7763|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_type_counts = df.where(col(\"event_type\") != '' ).groupBy(\"event_type\").count()\n",
    "\n",
    "# Muestra el resultado\n",
    "event_type_counts.orderBy(col('count').desc()).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+\n",
      "|event_type| count|        percentage|\n",
      "+----------+------+------------------+\n",
      "|      view|482642|           96.5284|\n",
      "|  purchase|  9595|1.9189999999999998|\n",
      "|      cart|  7763|            1.5526|\n",
      "+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cuenta la cantidad de ocurrencias de cada tipo de evento después de filtrar los no nulos\n",
    "event_type_counts = df.where(col(\"event_type\") != '').groupBy(\"event_type\").count()\n",
    "\n",
    "# Calcula el total de registros en el DataFrame original\n",
    "total_records = df.count()\n",
    "\n",
    "# Agrega una columna que representa el porcentaje\n",
    "event_type_counts_with_percentage = event_type_counts.withColumn(\n",
    "    \"percentage\",\n",
    "    (col(\"count\") / total_records) * 100\n",
    ")\n",
    "\n",
    "# Ordena los resultados por la columna \"count\" en orden descendente\n",
    "sorted_event_type_counts = event_type_counts_with_percentage.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Muestra el resultado ordenado (solo las 3 primeras filas)\n",
    "sorted_event_type_counts.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "user_id_counts = df.where(col(\"user_id\") != '' ).count()\n",
    "\n",
    "# Muestra el resultado\n",
    "print(user_id_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|    user_id|purchase_count|\n",
      "+-----------+--------------+\n",
      "|521734903.0|             1|\n",
      "|564133858.0|             5|\n",
      "|513114259.0|             1|\n",
      "|516995448.0|             1|\n",
      "|512691830.0|             1|\n",
      "|558710419.0|             1|\n",
      "|566302098.0|             2|\n",
      "|521793571.0|             5|\n",
      "|553458480.0|             1|\n",
      "|523955361.0|             2|\n",
      "|518956507.0|             1|\n",
      "|562067899.0|             1|\n",
      "|533189542.0|             1|\n",
      "|519048208.0|             1|\n",
      "|554459781.0|             1|\n",
      "|547422663.0|             1|\n",
      "|565824597.0|             1|\n",
      "|543455415.0|             1|\n",
      "|515412653.0|             1|\n",
      "|518277746.0|             1|\n",
      "+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "purchase_events = df.filter(df.event_type == \"purchase\")\n",
    "\n",
    "user_purchase_counts = purchase_events.groupBy(\"user_id\").agg(\n",
    "    F.count(\"event_type\").alias(\"purchase_count\")\n",
    ")\n",
    "\n",
    "user_purchase_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------+\n",
      "|months_since_first_purchase|retained_users|\n",
      "+---------------------------+--------------+\n",
      "|                          0|          7389|\n",
      "+---------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, min, expr\n",
    "\n",
    "first_purchase_dates = purchase_events.groupBy(\"user_id\").agg(min(\"event_time\").alias(\"first_purchase\"))\n",
    "user_retention = first_purchase_dates.join(purchase_events, on=\"user_id\").select(\n",
    "    \"user_id\",\n",
    "    (datediff(\"event_time\", \"first_purchase\") / 30).cast(\"int\").alias(\"months_since_first_purchase\")\n",
    ").distinct()\n",
    "\n",
    "retention_counts = user_retention.groupBy(\"months_since_first_purchase\").agg(\n",
    "    F.count(\"user_id\").alias(\"retained_users\")\n",
    ")\n",
    "\n",
    "retention_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
